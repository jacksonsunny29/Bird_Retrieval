{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b707838",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8a3dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f863d",
   "metadata": {},
   "source": [
    "# Dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d33d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ffaa0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pierr\\anaconda3\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d3b2fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdsDataset(Dataset):\n",
    "    \"\"\"Birds dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, processor, transforms = None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "        \"\"\"\n",
    "        self.birds_df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.processor = processor\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.birds_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.birds_df.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        labels = self.birds_df.iloc[idx, 1:]\n",
    "        labels = np.array([labels], dtype=float)-1\n",
    "        inp = processor(text=None, images=image, return_tensors=\"pt\", padding=True)\n",
    "        inp['pixel_values'] = torch.squeeze(inp['pixel_values'])\n",
    "        if transforms:\n",
    "            inp = self.transforms(inp)\n",
    "        sample = (inp.to(device),labels)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf7a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'C:\\\\Users\\\\pierr\\\\Documents\\\\INF649-Computer_Vision\\\\project\\\\data\\\\CUB_200_2011\\\\birds.csv'\n",
    "root_dir = 'C:\\\\Users\\\\pierr\\\\Documents\\\\INF649-Computer_Vision\\\\project'\n",
    "transforms = v2.Compose([v2.Resize((64,64))])\n",
    "\n",
    "birds_dataset = BirdsDataset(csv_file=csv_file,\n",
    "                             root_dir=root_dir,\n",
    "                             processor = processor,\n",
    "                             transforms = transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01706496",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset, testset = torch.utils.data.random_split(birds_dataset, (0.8, 0.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc62ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(trainset,\n",
    "                                             batch_size=500, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(valset, batch_size=1, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14e495",
   "metadata": {},
   "source": [
    "# Model and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e37ba3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class endClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, out_dim = 200, input_dim = 512, hidden_dim = 256, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_features=input_dim, out_features=hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(p=dropout))\n",
    "        layers.append(nn.Linear(in_features=hidden_dim, out_features=hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(p=dropout))\n",
    "        layers.append(nn.Linear(in_features=hidden_dim, out_features=out_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.layers.apply(self.init_weights)\n",
    "        \n",
    "    def forward(self, inp: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        takes as input CLIP image features (size 512) and returns classification score\n",
    "        \"\"\"\n",
    "        inp = self.layers(inp)\n",
    "        return inp  \n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32d99961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModel(nn.Module):\n",
    "    def __init__(self, clip_model, end_model = None, num_classes = 200, lr= 1e-3):\n",
    "        \"\"\"Initializing the model.\n",
    "\n",
    "        Args:\n",
    "            clip_model: CLIP model used as backbone for image feature generation\n",
    "            num_classes (int, optional): number of output classes. Defaults to 200.\n",
    "            learning_rate (float, optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        print(\"initializing classifier model...\")\n",
    "        self.num_classes = num_classes\n",
    "        self.clip_model = clip_model\n",
    "        \n",
    "        #freezing CLIP, we only want to train the linear layers we'll add at the end \n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        #adding linear layers\n",
    "        if end_model:\n",
    "            model = end_model\n",
    "        else:\n",
    "            model = endClassifier(out_dim=self.num_classes).to(device)\n",
    "        self.classifier = model\n",
    "            \n",
    "        # setting the loss\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        #for testing\n",
    "        self.preds = []\n",
    "        self.targs = []\n",
    "        print(\"classifier model initialized !\")\n",
    "    \n",
    "    def training_step(self, batch) -> torch.Tensor:\n",
    "        \"\"\"Training forward pass.\n",
    "\n",
    "        Args:\n",
    "            batch ([type]): input batch of images and its corresponding classes.\n",
    "\n",
    "        Returns:\n",
    "            loss [torch.Tensor]: training loss value.\n",
    "        \"\"\"\n",
    "        _x, _y = batch\n",
    "        _y = torch.squeeze(_y).long().to(device)\n",
    "        _z = self.clip_model.get_image_features(**_x)\n",
    "        _out = self.classifier(_z)\n",
    "        _loss = self.loss(_out, _y)\n",
    "\n",
    "        return _loss\n",
    "     \n",
    "    def validation_step(self, batch) -> torch.Tensor:\n",
    "        \"\"\"Validation forward step.\n",
    "\n",
    "        Args:\n",
    "            batch ([type]): input batch of images and its corresponding classes.\n",
    "        Returns:\n",
    "            loss [torch.FloatTensor]: validation loss value.\n",
    "        \"\"\"\n",
    "        _x, _y = batch\n",
    "        _y = torch.squeeze(_y).long().unsqueeze(0).to(device)\n",
    "        _z = self.clip_model.get_image_features(**_x)\n",
    "        _out = self.classifier(_z)\n",
    "        _loss = self.loss(_out, _y)\n",
    "\n",
    "        return _loss\n",
    "    \n",
    "    def test_step(self, batch) -> None:\n",
    "        \"\"\"Test step.\n",
    "\n",
    "        Args:\n",
    "            batch ([type]): input batch of images and its corresponding classes.\n",
    "        \"\"\"\n",
    "        _x, _y = batch\n",
    "        _y = torch.squeeze(_y).long().unsqueeze(0).to(device)\n",
    "        _z = self.clip_model.get_image_features(**_x)\n",
    "        _out = self.classifier(_z)\n",
    "        _loss = self.loss(_out, _y)\n",
    "        \n",
    "        self.targs.extend(_y.cpu().numpy())\n",
    "        self.preds.extend(torch.argmax(_out, dim=1).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ba5132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    def __init__(self, classifierModel, trainloader, valloader, testloader, lr):\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.testloader = testloader\n",
    "        self.ClassifierModel = classifierModel\n",
    "        \n",
    "        #optimizer\n",
    "        self.learning_rate = lr\n",
    "        self.optimizer = torch.optim.Adam(self.ClassifierModel.classifier.parameters(), lr=self.learning_rate)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=20, gamma=0.4)\n",
    "        \n",
    "    def train_one_epoch(self, epoch_index):\n",
    "        running_loss = 0.\n",
    "        last_loss = 0.\n",
    "\n",
    "        # Here, we use enumerate(trainloader) instead of\n",
    "        # iter(trainloader) so that we can track the batch\n",
    "        # index and do some intra-epoch reporting\n",
    "        for i, batch in enumerate(tqdm(self.trainloader)):\n",
    "\n",
    "            # Zero your gradients for every batch!\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Compute the loss and its gradients           \n",
    "            loss = self.ClassifierModel.training_step(batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning weights\n",
    "            self.optimizer.step()\n",
    "            # Changing learning rate\n",
    "            self.lr_scheduler.step()\n",
    "            # Gather data and report\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:\n",
    "                last_loss = running_loss / 10 # loss per batch\n",
    "                print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                running_loss = 0.\n",
    "\n",
    "        return last_loss\n",
    "    \n",
    "    def train_multiple_epochs(self, EPOCHS = 100):\n",
    "        epoch_number = 0\n",
    "        best_vloss = 1_000_000.\n",
    "        for epoch in range(EPOCHS):\n",
    "            print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "            # Make sure gradient tracking is on, and do a pass over the data\n",
    "            self.ClassifierModel.classifier.train(True)\n",
    "            avg_loss = self.train_one_epoch(epoch_number)\n",
    "\n",
    "            running_vloss = 0.0\n",
    "            # Set the model to evaluation mode, disabling dropout and using population\n",
    "            # statistics for batch normalization.\n",
    "            self.ClassifierModel.classifier.eval()\n",
    "\n",
    "            # Disable gradient computation and reduce memory consumption.\n",
    "            print('Epoch {} validation step'.format(epoch_number + 1))\n",
    "            with torch.no_grad():\n",
    "                for i, vdata in enumerate(tqdm(self.valloader)):\n",
    "                    vloss = self.ClassifierModel.validation_step(vdata)\n",
    "                    running_vloss += vloss\n",
    "\n",
    "            avg_vloss = running_vloss / (i + 1)\n",
    "            print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "            # Log the running loss averaged per batch\n",
    "            # for both training and validation\n",
    "\n",
    "            # Track best performance, and save the model's state\n",
    "            if avg_vloss < best_vloss:\n",
    "                best_vloss = avg_vloss\n",
    "                ClassifierModel_path = 'models\\\\BIRBModel_{}_{}'.format(timestamp, epoch_number)\n",
    "                torch.save(self.ClassifierModel.classifier.state_dict(), ClassifierModel_path)\n",
    "\n",
    "            epoch_number += 1\n",
    "\n",
    "    def test(self):       \n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        pred_labels = []\n",
    "        with torch.no_grad():\n",
    "            for i, tdata in enumerate(tqdm(self.testloader)):\n",
    "                self.ClassifierModel.test_step(tdata)\n",
    "                total_samples+=1\n",
    "                          \n",
    "        correct_preds = np.array(birb_model.targs)==np.array(birb_model.preds)\n",
    "        correct_preds = correct_preds.astype(int)\n",
    "        correct_predictions = np.sum(correct_preds)\n",
    "        accuracy = correct_predictions / total_samples\n",
    "        print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "birb_model = ClassifierModel(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd5127",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(birb_model, train_dataloader, val_dataloader, test_dataloader, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d6691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "trainer.train_multiple_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b9fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f375b",
   "metadata": {},
   "source": [
    "### Loading a model for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d78ff1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_model = endClassifier(out_dim=200).to(device)\n",
    "end_model.load_state_dict(torch.load(\"models\\\\BIRBModel_20240318_215922_50\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3aa4631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing classifier model...\n",
      "classifier model initialized !\n"
     ]
    }
   ],
   "source": [
    "birb_model = ClassifierModel(clip_model, end_model = end_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1ba35e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1178/1178 [00:45<00:00, 25.82it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, tdata \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(test_dataloader)):\n\u001b[0;32m      6\u001b[0m         birb_model\u001b[38;5;241m.\u001b[39mtest_step(tdata)\n\u001b[1;32m----> 8\u001b[0m correct_predictions \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mbirb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbirb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     10\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m correct_predictions \u001b[38;5;241m/\u001b[39m total_samples\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, tdata in enumerate(tqdm(test_dataloader)):\n",
    "        birb_model.test_step(tdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36525277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6774193548387096"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(birb_model.targs)==np.array(birb_model.preds)\n",
    "a.astype(int)\n",
    "np.sum(a)/1178"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
